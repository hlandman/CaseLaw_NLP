{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning for Automatic Case Law Summarization\n",
    "\n",
    "Shashi Kunapuli, Hillel Landman, Saif Mohammed & Khurram Munawar\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "from spacy.cli import link\n",
    "from spacy.util import get_package_path\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, Bidirectional, concatenate\n",
    "from tensorflow.keras.layers import TimeDistributed, RepeatVector, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, GPT2Model\n",
    "\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scope of this project is to produce automated summarization of Case Law by utilizing extractive or abstractive NLP models. Case law is a collection of legal decisions that are used in future cases to determine legal precedents. \n",
    "\n",
    "The <a href=\"https://case.law/\">Caselaw Access Project</a> files compiled by Harvard Law School is a corpus of three hundered years worth of Case Law documentation in the United States. For this project North Carolina Case Law files were used as source data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## II) Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### 1) Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset loaded for this project contains over 97,000 case law texts from North Carolina since the early 1800s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = []\n",
    "with open(\"data/dataNC2.jsonl\") as f:\n",
    "    for line in f:\n",
    "        data2.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain hyperparameters are established for when the data is preprocessed. Cases are filtered to those only as far back as 1980 in order to capture language that is more relevant to today, and also to reduce processing time for certain functions. Cases are also filtered to include those with a minimum word length of 500 words, and headnotes with a minimum of 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECENCY_YEAR = 1980\n",
    "MIN_WORD_LEN = 500\n",
    "MIN_HEADNOTE_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Filter recent cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent cases are filtered by reading in the year as a datetime object and appending those from 1980 and later to our `recents` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [d['decision_date'] for d in data2]\n",
    "\n",
    "recents = []\n",
    "date_errors = []\n",
    "for i, date in enumerate(dates):\n",
    "    try:\n",
    "        if datetime.strptime(date, '%Y-%m-%d').year >= RECENCY_YEAR:\n",
    "            recents.append(data2[i])\n",
    "    except ValueError:\n",
    "        date_errors.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are just over 38,000 cases from 1980 and later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Extract text elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of the dataset is formatted as a python dictionary. Within the dictionary, a `data` element contains an XML-formatted body that includes most of the relevant information. Each case includes the case text itself and a section of headnotes. We parse the `data` using BeautifulSoup, extracting texts and headnotes that meet the minimum length hyperparameter specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 37s, sys: 55.1 ms, total: 10min 37s\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "# Extract raw text elements if body is long enough\n",
    "raw_text = []\n",
    "raw_info = []\n",
    "raw_headnotes = []\n",
    "errors = 0\n",
    "\n",
    "for i in range(len(recents)):\n",
    "    try:\n",
    "        d = recents[i]\n",
    "        soup = BeautifulSoup(d['casebody']['data'])\n",
    "        \n",
    "        # Check if text/headnotes are long enough\n",
    "        if len(word_tokenize(' '.join([element.text for element in soup.find_all('p')]))) >= MIN_WORD_LEN:\n",
    "            if len(word_tokenize(' '.join([element.text for element in soup.find_all('headnotes')]))) >= MIN_HEADNOTE_LEN:\n",
    "                \n",
    "                # Append text for each case/headnotes\n",
    "                raw_text.append(' '.join([element.text for element in soup.find_all('p')]))\n",
    "                raw_headnotes.append(' '.join([element.text for element in soup.find_all('headnotes')]))\n",
    "\n",
    "                # Append other info\n",
    "                raw_info.append([d['name'], d['decision_date'], d['court']['name']])\n",
    "        \n",
    "    except IndexError:\n",
    "        errors += 1\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this process, just over 19,000 cases remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Cases: 19072\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of Cases: {}'.format(len(raw_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Preprocess the Data\n",
    "\n",
    "To preprocess the data, a preprocessing function is applied. The function accomplishes the following:\n",
    "\n",
    "- Removes special characters\n",
    "- Removes periods in the middle of sentences\n",
    "- Removes digits\n",
    "- Removes single-letter \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(text):\n",
    "    txt = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    txt = re.sub('\"','', txt)\n",
    "    txt = re.sub(r\"'s\\b\",\"\",txt)\n",
    "    txt = re.sub(\"\\n\",\"\",txt)\n",
    "    sents = sent_tokenize(txt)\n",
    "    sents = [re.sub('\\.(?!$)', ' ', i) for i in sents] # remove periods in middle of sentences\n",
    "    txt = \" \".join(sents).strip()\n",
    "    txt = re.sub(\"\\\\d\", \"\", txt) # remove numbers\n",
    "    tokens = [w for w in txt.split() if len(w) > 1] # remove single letters\n",
    "    return (\" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is applied to the case texts and headnotes to preprocess all of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 0 ns, total: 2min 28s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "# Create cleaned, preprocessed text and summaries/headnotes\n",
    "clean_text = []\n",
    "clean_text.append([text_preprocessor(txt) for txt in raw_text])\n",
    "clean_text = clean_text[0]\n",
    "\n",
    "clean_headnotes = []\n",
    "clean_headnotes.append([text_preprocessor(txt) for txt in raw_headnotes])\n",
    "clean_headnotes = clean_headnotes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data elements are then saved into memory for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 120 ms, total: 524 ms\n",
      "Wall time: 525 ms\n"
     ]
    }
   ],
   "source": [
    "# Write cleaned text into memory\n",
    "with open(os.path.join('data', 'clean_text.txt'), 'w') as f:\n",
    "    for line in clean_text:\n",
    "        f.write('%s\\n' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned headnotes into memory\n",
    "with open(os.path.join('data', 'clean_headnotes.txt'), 'w') as f:\n",
    "    for line in clean_headnotes:\n",
    "        f.write('%s\\n' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull cleaned text from memory\n",
    "clean_text = []\n",
    "\n",
    "with open(os.path.join('data', 'clean_text.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        current = line[:-1]\n",
    "        clean_text.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull cleaned headnotes from memory\n",
    "clean_headnotes = []\n",
    "\n",
    "with open(os.path.join('data', 'clean_headnotes.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        current = line[:-1]\n",
    "        clean_headnotes.append(current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Optional - Subset data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier on, when testing various models, subsets of the data were created and used to reduce training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data\n",
    "case_subset = clean_text[-4000:]\n",
    "headnotes_subset = clean_headnotes[-4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two methodologies for text summarization and both were looked at and experimented with in this project.\n",
    "1. Extractive Summarization\n",
    "2. Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractive summarization is achieved through selecting the most important sentences or words to the essence of the text. Based on their weights, these words or sentences are then used to compose the summary text. There are multiple methods to quantify sentence weights and subsequent importance ranking. The process flow of an extractive summarization technique in general is as follows:\n",
    "\n",
    "1. Text cleaning and pre-processing\n",
    "2. Extractive algorithm clusters sentences on similarity and generates sentence weights\n",
    "3. Select higher ranking sentences to compose extractive summaries\n",
    "\n",
    "Extractive summarization methods usually provide good results as they do not have to quantify semantic representation, inference or natural language generation. However, there are limitations to this approach as well. Some of them are listed below and were also observed in this project while experimenting with extractive summarization model.\n",
    "\n",
    "1. Extracted sentences usually tend to be longer than average which causes summaries to include part of text which may not be relevant.\n",
    "2. Extractive summaries are prone to omit relevant information if they are spread out among multiple sentences in different portions of the document. \n",
    "3. If the text contains conflicting information such as dissenting notes in a case law, that information may not be captured properly or may even cause inaccuracies in extractive summaries.\n",
    "4. Extractive summaries also suffer problems with overall coherency of the summary. This could be due extraction of words out of context. Also stitching of decontextualized words or sentences may cause incorrect interpretation of the actual documents in their summaries. \n",
    "\n",
    "These limitations could be addressed by extensive pre-processing of input data and/or post processing the extractive summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Gensim - Summaries of Full Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is an open source ML library for natural language and text processing. It works with word vector models such as Word2Vec. Gensim uses the TextRank algorithm through its summarize() function to summarize text string data. \n",
    "\n",
    "TextRank is a graph-based ranking algorithm for NLP. It is an extractive and unsupervised text summarization technique, which essentially ranks the importance of sentences/words depending on the number of interconnections, or \"voting.\" TextRank creates vector representations for every sentence and ranks them based on the similarities between sentence vectors in a graph format, where sentences are represented as vertices and the ranking as edges. Top ranked sentences are then drawn for the summary generation.\n",
    " \n",
    "The summarized output length from Gensim summarization is governed by Summarization Ratio or Maximum Count of Words hyperparameters. \n",
    "\n",
    "In this project Maximum Count of Words was used to maintain a constant word length in the summary for comparative analysis with gold standard baseline summaries as well as other alternate models implemented in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive summary using gensim's TextRank implementation\n",
    "gensim_summaries = []\n",
    "for i, tx in enumerate(clean_text[1:6]):\n",
    "    gensim_summaries.append(summarize(tx, word_count=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractive sumarization output was generated to check the coherence of a typical Gensim summary. The following printed results are from the first five cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim Extractive Summary for Case # 1\n",
      "searches conducted outside the judicial process, without prior approval by judge or magistrate, are per se unreasonable under the Fourth Amendment subject only to few specifically established and well-delineated exceptions We are of the opinion that the seizure of these letters and photographs which were not listed on the face of the warrant and therefore seized without prior judicial approval, was proper as coming within just such well-delineated exception; that of “plain view The “plain view” exception was discussed by the United States Supreme Court in Coolidge New Hampshire, Ed. Ct.\n",
      "\n",
      "Gensim Extractive Summary for Case # 2\n",
      "In that case, the Court held that the trial judge did not commit reversible error by permitting further examination and challenge of juror by the State after the jury was impaneled, when it was discovered that the juror worked with the wife of one of the defendants.\n",
      "\n",
      "Gensim Extractive Summary for Case # 3\n",
      "We have concluded, however, that rate adjustment pursuant to an annual CTR “true up” is not change in fixed general rate, and thus the rate adjustment in this case which allowed NCNG to offset its overcollection by its previous undercollection does not *constitute retroactive rate making prohibited by Utilities Commission Edmisten, E.\n",
      "\n",
      "Gensim Extractive Summary for Case # 4\n",
      "“Authority to do an act on the principal’s behalf does not ordinarily carry with it an implied authority to talk about it afterwards Stansbury, supra, at [] Application of the above principles to the facts in this case leads us to conclude that the statements allegedly made to plaintiff by defendant’s agent, Rochelle, were erroneously admitted into evidence.\n",
      "\n",
      "Gensim Extractive Summary for Case # 5\n",
      "[] Defendant’s first argument is that he was denied due process of law when the trial judge refused to grant him an evidentiary *hearing upon his motion to dismiss all of the charges.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, summ in enumerate(gensim_summaries):\n",
    "    print(\"Gensim Extractive Summary for Case # {}\".format(i+1))\n",
    "    print(summ)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Other Applications of Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Summaries of Headnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headnotes sections varied significantly in word length, ranging between 5 and almost 7000 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count = []\n",
    "for i in clean_headnotes:\n",
    "    word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'Words by Headnote Summary':word_count})\n",
    "length_df.hist(bins = 200)\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Count\")\n",
    "#plt.xlim((0, 300))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest Headnotes: 5 Words\n",
      "Longest Headnotes: 6290 Words\n"
     ]
    }
   ],
   "source": [
    "print(\"Shortest Headnotes: {} Words\".format(min(word_count)))\n",
    "print(\"Longest Headnotes: {} Words\".format(max(word_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, when seq2seq models are trained, headnotes need to be of similar length to standardize inputs. In order to stadardize headnotes length, Gensim's summarizer is applied to extract the most important sentences around 50 words from the headnotes, and are then stored in `clean_summaries`. These are used as target inputs for the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive summary using gensim's TextRank implementation\n",
    "clean_summaries = []\n",
    "for i, tx in enumerate(clean_headnotes):\n",
    "    try:\n",
    "        summ = summarize(tx, word_count=50)\n",
    "        if len(summ) < 10:\n",
    "            clean_summaries.append(tx)\n",
    "        else:\n",
    "            clean_summaries.append(summ)\n",
    "    except ValueError:\n",
    "        clean_summaries.append(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Preprocessing for Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractive summarization is also used to shorten case inputs for pre-trained models. The GPT-2 model, in particular, has a built-in maximum length of 1024 words per input. Gensim's summarizer is applied to shorten inputs by extracting only the most important sentences. Cases above 8000 words are also removed, because extraction took a very long time for those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_too_long = []\n",
    "for i in clean_text:\n",
    "    if len(i.split()) < 8000:\n",
    "        not_too_long.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18788"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_too_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_preprocess(text, max_words, tag='TL;DR:'):\n",
    "    '''\n",
    "    Returns shortened text based on Gensim's TextRank algorithm.\n",
    "    Optionally tags texts for summarization in gpt2\n",
    "    '''\n",
    "    # Extractive summary of max_words words\n",
    "    if len(text.split()) > max_words:\n",
    "        short = summarize(text, word_count=max_words)\n",
    "    else:\n",
    "        short = text\n",
    "    \n",
    "    # Tag for summarization\n",
    "    if tag != None:\n",
    "        short = ' '.join((short, tag))\n",
    "    \n",
    "    return short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 46s, sys: 35 s, total: 25min 21s\n",
      "Wall time: 15min 25s\n"
     ]
    }
   ],
   "source": [
    "short_cases = [gpt2_preprocess(i, 500) for i in not_too_long]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'short_cases2.txt'), 'w') as f:\n",
    "    for line in short_cases:\n",
    "        f.write('%s\\n' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortcases = []\n",
    "\n",
    "with open(os.path.join('data', 'short_cases2.txt'), 'r') as f:\n",
    "    shortcases.append(f.read().split('TL;DR:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortcases = shortcases[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Abstractive Summarization new sentences are generated to capture the meaning of the source document and hence is more complex as compared to the extractive approach. Abstractive techniques have the potential to produce human-like textual summaries, since it supposed to capture semantic representation, inference and implement natural language generation. This removes the constraints on the model to use pre-written text. Abstractive models do need large-scale domain specific data for training.\n",
    "\n",
    "Word embeddings may need to be learned to capture semantical, hierarchical and contextual information from the source documents and may greatly impact the accuracy of an abstractive model. For this project GloVe Wikipedia 2014 + Gigaword 5 Word embeddings were used, since training custom word embedding for the Case Law data set would be resource- and time-intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building a word embeddings matrix, max sequence lengths must be determined and hyperprameters must be set. Max sequence lengths were set at 2500 for the input cases and 50 for the summaries, since the majority of cases were around those lengths or shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count = []\n",
    "for i in clean_text:\n",
    "    word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'Words by Case':word_count})\n",
    "length_df.hist(bins = 100)\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlim((0, 8000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count = []\n",
    "for i in clean_summaries:\n",
    "    word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'Words by Headnote Summary':word_count})\n",
    "length_df.hist(bins = 200)\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlim((0, 300))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_CASE_LENGTH = 2500\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EMBEDDING_DIM = 100\n",
    "EMB_DIR = 'embeddings'\n",
    "MAX_SUMMARY_LENGTH = 50\n",
    "RANDOM_STATE = 109\n",
    "GLOVE_FILE = 'glove.6B.{}d.txt'.format(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into train and test groups so that they can be set up data for embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation datasets\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(clean_text, \n",
    "                                            clean_summaries, \n",
    "                                            test_size=VALIDATION_SPLIT, \n",
    "                                            random_state=RANDOM_STATE, \n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word tokenizer is then prepared to convert words to integer sequences and later embed them. Sequences are padded with zeroes at the end so that they are all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for cases on training data\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts([i for i in x_tr])\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr = pad_sequences(x_tr,  maxlen=MAX_CASE_LENGTH, padding='post') \n",
    "x_val = pad_sequences(x_val, maxlen=MAX_CASE_LENGTH, padding='post')\n",
    "\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y_tr = x_tokenizer.texts_to_sequences(y_tr)\n",
    "y_val = x_tokenizer.texts_to_sequences(y_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr = pad_sequences(y_tr, maxlen=MAX_SUMMARY_LENGTH, padding='post')\n",
    "y_val = pad_sequences(y_val, maxlen=MAX_SUMMARY_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = x_tokenizer.word_index\n",
    "index_word = x_tokenizer.index_word\n",
    "\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "index_word[0] = 'ENDPAD'\n",
    "word_index['ENDPAD'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding matrix is then created based on <a href=\"http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\">GloVe Wikipedia 2014 + Gigaword 5</a> embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Embedding Layer\n",
    "# GloVe Wikipedia 2014 + Gigaword 5 embeddings\n",
    "# from http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(EMB_DIR, GLOVE_FILE))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a Keras `embedding_layer` containing these word embedding weights is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_CASE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular deep-learning algorithms used for natural language processing is Seq2Seq. This methodology uses recurrent neural networks containing one encoder and one decoder. In this model, the encoder network accepts case law text as input, creates word embeddings using the previously-created Keras `embedding_layer`, and passes those through an LSTM. The decoder network accepts the encoder output as its input, and itself outputs summary text, word by word, based on the word it determines is most likely to come next. The existing summary text is repeatedly fed back into the decoder along with the latent space output of the text encoder, and those inputs are passed through the decoder to calculate each successive word.  \n",
    "\n",
    "A diagram of the general structure of the model used can be seen <a href=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Alternate-3-Recursive-Text-Summarization-Model-B.png\">below</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Alternate-3-Recursive-Text-Summarization-Model-B.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Alternate-3-Recursive-Text-Summarization-Model-B.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, adapted from \"Recursive Model B\" from <a href=\"https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/\">this site</a>, is instantiated and compiled, then trained for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Alternate 3: Recursive Model B\"\n",
    "# from https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/\n",
    "# article input model\n",
    "inputs1 = Input(shape=(MAX_CASE_LENGTH,), name='input1')\n",
    "\n",
    "article1 = embedding_layer(inputs1)\n",
    "article2 = LSTM(EMBEDDING_DIM)(article1)\n",
    "article3 = RepeatVector(MAX_SUMMARY_LENGTH)(article2)\n",
    "\n",
    "# summary input model\n",
    "inputs2 = Input(shape=(MAX_SUMMARY_LENGTH,), name='input2')\n",
    "summ1 = embedding_layer(inputs2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = concatenate([article3, summ1])\n",
    "decoder2 = LSTM(EMBEDDING_DIM, return_sequences=True)(decoder1)\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder2)\n",
    "\n",
    "# tie it together [article, summary] [word]\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs, name=\"Headnotes\")\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Headnotes\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input1 (InputLayer)             [(None, 2500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           multiple             13642700    input1[0][0]                     \n",
      "                                                                 input2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 100)          80400       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input2 (InputLayer)             [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 50, 100)      0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 50, 200)      0           repeat_vector[0][0]              \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50, 100)      120400      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 50, 136427)   13779127    lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 27,622,627\n",
      "Trainable params: 13,979,927\n",
      "Non-trainable params: 13,642,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit([x_tr,y_tr], y_tr,\n",
    "                  epochs=5,batch_size=256, \n",
    "                  validation_data=([x_val,y_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8352d93790>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saved weights\n",
    "model.load_weights(\"weights/on_headnotes\")\n",
    "\n",
    "# Other model weights\n",
    "#model.load_weights(\"weights/model_recursive\")\n",
    "#model.load_weights(\"weights/glove_recursive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, a function is built to produce summary predictions based on the model. In this particular case, the prediction function is told to skip over stopwords. Since stopwords are so common, they are often identified as \"most likely\" words even when not appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_prediction(case_idx, max_len=MAX_CASE_LENGTH, max_sum_len=MAX_SUMMARY_LENGTH):\n",
    "    '''\n",
    "    Function to output predicted summary.\n",
    "    \n",
    "    case_idx is index for predicted case.\n",
    "    \n",
    "    max_len and max_sum_len are hyperparameters\n",
    "    '''\n",
    "    # Set up input and target sequences\n",
    "    input_seq = x_val[case_idx].reshape(1, max_len)\n",
    "    target_seq = np.zeros((1, max_sum_len))\n",
    "    \n",
    "    # Stopwords from nltk\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    sentence = []\n",
    "\n",
    "    for i in range(max_sum_len):\n",
    "        \n",
    "        # Predict likelihood of each word\n",
    "        pred = model.predict([input_seq, target_seq])\n",
    "        \n",
    "        # Choose next likeliest word if it is a stopword\n",
    "        stopword = True\n",
    "        ind = 0\n",
    "        while stopword:\n",
    "            sampled_token_index = np.argsort(-pred[0, i, :])[ind]+1\n",
    "            ind += 1\n",
    "            word = index_word[sampled_token_index]\n",
    "            if word not in stop_words:\n",
    "                if (i == 0 or word != sentence[i-1]):\n",
    "                    stopword = False\n",
    "        \n",
    "        sentence.append(word)\n",
    "        target_seq[0, i] = sampled_token_index\n",
    "\n",
    "    return ' '.join([i for i in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary predictions are printed for 3 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial\n",
      "\n",
      "defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial\n",
      "\n",
      "defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial defendant trial\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    print(seq2seq_prediction(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions do not even come close to coherent sentences, let alone anything resembling a case summary.  \n",
    "\n",
    "There are two major, apparent issues with these predictions, which both stem from the same set of model limitations. The first issue is that the words \"defendant\" and \"trial\" are just repeated over and over again. The model has identified these as the most likely next words, regardless of the context of the existing summary text that gets repeatedly input into the model. The prediction function manually ensures that no single word is repeated one after the other, but it does not ensure that words do not repeat alternately.  \n",
    "\n",
    "The second major issue is that the predictions are exactly the same, regardless of the input text. Any case input into the model will have a predicted summary of \"defendant trial defendant trial... etc\".  \n",
    "\n",
    "The Seq2Seq model works by predicting the most likely next summary word, given an input text and the existing summary sequence. Clearly, this model is grossly over-generalized. There are various ways to improve this model, including:\n",
    "- Increasing training data size\n",
    "- Increasing layer parameters\n",
    "- Adding additional hidden LSTM layers\n",
    "- (And most importantly) Increasing training time\n",
    "\n",
    "As it is, the Seq2Seq model took 5-10 hours to train. To improve the model to anything resembling coherency, training time would likely increase exponentially. Given the time and resources allocated for this project, attempts to train an adequate model were put on hold. Instead, focus was shifted to testing and optimizing existing, pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "#### 2) Pretrained Model: BertSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href = https://arxiv.org/abs/1903.10318>BertSum</a> is a pre-trained text summarization model on the <a href=https://cs.nyu.edu/~kcho/DMQA/>CNN/Daily Mail</a> news text corpus. BertSum uses a greedy method to select sentences which can maximize ROUGE scores in comparison to the baseline summaries. BertSum scores the sentences' representativeness of the source document and uses high-scoring sentences for document summarization.\n",
    "\n",
    "BertSum is a modification of BERT, which is a pretrained language model focused on tokens/words rather than sentences. Since sentence representation is necessary for text summarization tasks, BERT is modified to capture features and representations of each sentence and to distinguish multiple sentences along with their hierarchy in the source document.\n",
    "\n",
    "One of the drawbacks for using BertSum for Case Law is the fact that it is trained on news text and may not contain the semantics and contextual essence of legal documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the data using BertSum, nlpyang's <a href=https://github.com/nlpyang/PreSumm>PreSumm</a> pipeline is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/mingchen62/PreSumm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown https://drive.google.com/uc?id=1kKWoV0QCbeIuFt85beQgJ4v0lujaXobJ&export=download #CNN/DM Extractive bertext_cnndm_transformer.pt\n",
    "\n",
    "# Downloaded/unzipped locally\n",
    "# !gdown https://drive.google.com/uc?id=1-IKVCtc4Q-BdZpjXc4s70_fRsWnjtYLr&export=download #CNN/DM Abstractive model_step_148000.pt \n",
    "\n",
    "# !unzip models/bertsumextabs_cnndm_final_model.zip\n",
    "\n",
    "# !mkdir PreSumm/models/CNN_DailyMail_Abstractive\n",
    "# !mv models/model_step_148000.pt models/CNN_DailyMail_Abstractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(os.path.join(d_path, 'case1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = 'PreSumm/bert_data/cnndm'\n",
    "\n",
    "with open(os.path.join(d_path, 'case1.txt'), 'w') as f:\n",
    "    f.write(shortcases[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertSum is then executed from command line to generate summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 summarizer.py -task abs -mode test -test_from ../models/CNN_DailyMail_Abstractive/model_step_148000.pt -batch_size 32 -test_batch_size 500 -bert_data_path ../bert_data/cnndm -log_file ../logs/val_abs_bert_cnndm -report_rouge False  -sep_optim true -use_interval true -visible_gpus -1 -max_pos 512 -max_src_nsents 100 -max_length 50 -alpha 0.95 -min_length 20 -result_path ../results/abs_bert_cnndm_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_file = 'PreSumm/results/abs_bert_cnndm_sample.148000.candidate'\n",
    "summ_path = 'BertSum_Summaries'\n",
    "\n",
    "with open(summ_file, 'r') as f:\n",
    "    with open(os.path.join(summ_path, 'summary5.txt'), 'w') as g:\n",
    "        g.write(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ipynb checkpoints folder if necessary\n",
    "ipath = os.path.join(summ_path, '.ipynb_checkpoints')\n",
    "for file in os.listdir(ipath):\n",
    "    os.remove(os.path.join(file, ipath))\n",
    "os.rmdir(ipath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for i in range(1,6):\n",
    "    filenames.append('summary{}.txt'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertSum summaries of the first five cases are printed out to verify coherency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertSum Summary for Case # 1\n",
      "danny cevallos : the only question for review is the admissibility of letters and photographs seized by deputies of the onslow county sheriff 's department during search of the mobile home occupied by the defendants<q>he says the seizure of these letters and\n",
      "\n",
      "BertSum Summary for Case # 2\n",
      "the trial judge did not commit reversible error by permitting further examination and challenge of juror by the state after the jury was impaneled<q>mrs trogdon testified on voir dire that after she had seen defendant at the restaurant in september\n",
      "\n",
      "BertSum Summary for Case # 3\n",
      "the court of appeals upheld the use of formula known as the volume variation adjustment factor which provided for the yearly \" true up \" of natural gas in manner identical to that of the ctr<q>in the case , ncng can not know the actual rate\n",
      "\n",
      "BertSum Summary for Case # 4\n",
      "the dispositive question on this appeal is whether statements allegedly made to plaintiff by defendant 's agent rochelle were properly admissible into evidence as the admissions of southern bell<q>if there is no competent evidence that an agent has authority to speak for his\n",
      "\n",
      "BertSum Summary for Case # 5\n",
      "defense counsel produced statistics tending to show that other defendants had charges dismissed when the prosecuting witness so desired<q>if these statistics alone were enough to establish denial of equal protection , mandatory rule would be created requiring the district attorney to dismiss charges in all cases where\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summ_path = 'BertSum_Summaries'\n",
    "\n",
    "BertSum_summaries = []\n",
    "for i, file in enumerate(filenames):\n",
    "    with open(os.path.join(summ_path, file)) as f:\n",
    "        BertSum_summaries.append(f.read())\n",
    "    print('BertSum Summary for Case # {}'.format(i+1))\n",
    "    print(BertSum_summaries[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Pretrained Model: GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=https://openai.com/blog/better-language-models/>GPT-2</a> is a state-of-the-art text generation model with the capability to produce long texts. GPT-2 uses auto-regression, meaning it outputs one word at a time, which is then appended at the end of the input words to create a new input sequence to generate the next word output. \n",
    "\n",
    "GPT-2 uses an extremely large number of parameters and is trained on millions of web pages. Due to this diversity the expectation is that GPT-2 is capable of generating text with human level accuracy and quality.\n",
    "\n",
    "GPT-2 also has a text summarization feature. For this project, two major limitations were observed duirng its implementation. The first limitation is that the maximum number of input word tokens is set to 1024, whereas some cases have thousands of words. To deal with this, Gensim's extractive summarizer was leveraged (see \"Extractive Summarization\" section 2b) to reduce cases to their most important sentences. \n",
    "\n",
    "The second limitation is that the GPT-2 summarizer model is, in many cases, prone to repeating phrases and/or sentences.\n",
    "\n",
    "These limitations can be mitigated by training a new model with custom configurations, including increasing the maximum number of tokens, and setting a \"temperature\" parameter to decrease the likelihood of repetition. In future work, this could enhance the usability and performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_summarize(case, max_summ_len=50):\n",
    "    '''\n",
    "    Uses gpt2 to implement abstractive summarization\n",
    "    '''\n",
    "    # Instantiate tokenizer and model from pre-trained GPT2\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Tokenize text and convert to tensor\n",
    "    generated = []\n",
    "    input_ids = tokenizer.encode(case, add_special_tokens=True)\n",
    "    context = torch.tensor([input_ids]).unsqueeze(0)\n",
    "    past = None\n",
    "\n",
    "    # Generate summary words iteratively\n",
    "    for i in range(max_summ_len):\n",
    "        output, past = model(context, past=past)\n",
    "        token = torch.argmax(output[..., -1, :])\n",
    "\n",
    "        generated += [token.tolist()]\n",
    "        context = token.unsqueeze(0)\n",
    "    \n",
    "    # Reverse-tokenize generated sentence\n",
    "    sequence = tokenizer.decode(generated)\n",
    "\n",
    "    # Return first sentence\n",
    "    return sequence.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "gpt2_summaries = []\n",
    "\n",
    "for i in range(1,6):\n",
    "    try:\n",
    "        summary = gpt2_summarize(' '.join((clean_text[i], 'TL;DR:')))\n",
    "    except:\n",
    "        summary = gpt2_summarize(' '.join((shortcases[i], 'TL;DR:')))\n",
    "    gpt2_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_path = 'gpt2_summaries'\n",
    "for i, summ in enumerate(gpt2_summaries):\n",
    "    with open(os.path.join(gpt2_path, filenames[i]), 'w') as f:\n",
    "        f.write(summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five cases of GPT-2 generated summaries were printed to verify if they are coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 Summary for Case # 1\n",
      " The Court of Appeals for the Ninth Circuit, in its opinion, held that the search of the mobile home of the defendants was not per se unreasonable under the Fourth Amendment\n",
      "\n",
      "GPT2 Summary for Case # 2\n",
      " The State was not required to prove the charge of rape in order to convict defendant of burglary\n",
      "\n",
      "GPT2 Summary for Case # 3\n",
      "\n",
      "The Court of Appeals held that the Commission's decision to reduce the amount of the “true up” of NCNG's past undercollection by NCNG was not change in fixed general rate, and therefore the Commission's decision to reduce the\n",
      "\n",
      "GPT2 Summary for Case # 4\n",
      " The majority's opinion is correct\n",
      "\n",
      "GPT2 Summary for Case # 5\n",
      " The trial judge stated that the defendant was not entitled to counsel during the sentencing phase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(filenames):\n",
    "    with open(os.path.join(gpt2_path, file)) as f:\n",
    "        print('GPT2 Summary for Case # {}'.format(i+1))\n",
    "        print(f.read())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation with Blackstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the issues with using pre-trained models for case law is that those models are trained on a wide array of corpuses, while case law uses very specific and often unique language. A potential method of mitigating this is through data augmentation. If certain elements within the input texts are weighted higher, it is possible that pre-trained models will identify them as more important and incorporate that into their outputs.  \n",
    "\n",
    "<a href=https://github.com/ICLRandD/Blackstone>Blackstone</a> is a legal NLP tool developed by the Incorporated Council of Law Reporting for England and Wales. More specifically, it is \"a spaCy model and library for processing long-form, unstructured legal text.\" The model is designed to accept legal texts as inputs, and output classifications for each sentence as one of the following:\n",
    "\n",
    "| Category | Description |\n",
    "| --- | --- |\n",
    "| AXIOM\t| The text appears to postulate a well-established principle |\n",
    "| CONCLUSION | The text appears to make a finding, holding, determination or conclusion |\n",
    "| ISSUE\t| The text appears to discuss an issue or question |\n",
    "| LEGAL_TEST | The test appears to discuss a legal test |\n",
    "| UNCAT | The text does not fall into one of the four categories above |\n",
    "  \n",
    "In this section, Blackstone is leveraged to identify important sentences in legal texts. Once important sentences are identified, case texts can be augmented either by filtering only important sentences, or by repeating sentences with specific category tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Blackstone model is loaded through spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/share/anaconda3/lib/python3.7/site-packages/en_blackstone_proto -->\n",
      "/usr/share/anaconda3/lib/python3.7/site-packages/spacy/data/en_blackstone_proto\n",
      "You can now load the model via spacy.load('en_blackstone_proto')\n"
     ]
    }
   ],
   "source": [
    "model_name = \"en_blackstone_proto\"\n",
    "package_path = get_package_path(model_name)\n",
    "link(model_name, model_name, force=True, model_path=package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_blackstone_proto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline is created to process legal texts and output the most likely tag for each sentence, plus a confidence level for the most likely tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blackstone.pipeline.abbreviations import AbbreviationDetector\n",
    "from blackstone.pipeline.compound_cases import CompoundCases\n",
    "from blackstone.pipeline.sentence_segmenter import SentenceSegmenter\n",
    "from blackstone.rules import CITATION_PATTERNS\n",
    "\n",
    "# Add tools to blackstone pipeline\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "nlp.add_pipe(abbreviation_pipe)\n",
    "\n",
    "compound_pipe = CompoundCases(nlp)\n",
    "nlp.add_pipe(compound_pipe)\n",
    "\n",
    "sentence_segmenter = SentenceSegmenter(nlp.vocab, CITATION_PATTERNS)\n",
    "nlp.add_pipe(sentence_segmenter, before=\"parser\")\n",
    "\n",
    "def ents_cats(case):\n",
    "    \"\"\"\n",
    "    Function to identify entities in text and the highest scoring category\n",
    "    prediction generated by the text categoriser. \n",
    "    \"\"\"\n",
    "    doc = nlp(case)\n",
    "    \n",
    "    # Identify entities in text\n",
    "    entities = dict()\n",
    "    for ent in doc.ents:\n",
    "        entities[ent.text] = [ent.label_]\n",
    "    \n",
    "    # Identify most likely category, with score\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    categories = dict()\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        cats = doc.cats\n",
    "        score_list = sorted(list(cats.values()), reverse=True)\n",
    "        max_score = score_list[0]\n",
    "        second_score = score_list[1]\n",
    "        max_cats = [k for k, v in cats.items() if v == max_score]\n",
    "        max_cat = max_cats[0]\n",
    "        second_cats = [k for k, v in cats.items() if v == second_score]\n",
    "        second_cat = second_cats[0]\n",
    "        categories[sentence] = (max_cat, np.round(max_score, decimals=3)), (second_cat, np.round(second_score, decimals=3))\n",
    "    \n",
    "    return entities, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cats = ents_cats(clean_text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this model is not particularly effectve for this project's dataset. In the case printed below, for example, every sentence is predicted as UNCAT with a relatively high degree of confidence. This issue may be due to the fact that Blackstone was trained on UK legal texts, while the cases used in this project are all from the US. Blackstone therefore cannot be relied upon for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sole question for review by this Court is the admissibility of letters and photographs seized by deputies of the Onslow County Sheriff’s Department during search of the mobile home occupied by the defendants.\n",
      "(('UNCAT', 0.621), ('ISSUE', 0.361))\n",
      "\n",
      "The deputies searched the mobile home pursuant to validly issued “occupant warrant” which specified heroin as the object of the search.\n",
      "(('UNCAT', 0.998), ('CONCLUSION', 0.001))\n",
      "\n",
      "From the trailer’s bathroom, substance later determined to be heroin was seized, and after the heroin was discovered, letters and photographs which had been seen earlier were also taken from the adjoining bedroom.\n",
      "(('UNCAT', 0.997), ('CONCLUSION', 0.002))\n",
      "\n",
      "For the reasons which follow, we hold that the letters and photographs, though not specifically listed on the warrant as objects of the search, were properly seized and admitted into evidence.\n",
      "(('UNCAT', 0.914), ('CONCLUSION', 0.079))\n",
      "\n",
      "In Katz United States, Ed.\n",
      "(('UNCAT', 1.0), ('ISSUE', 0.0))\n",
      "\n",
      "Ct.\n",
      "(('UNCAT', 1.0), ('ISSUE', 0.0))\n",
      "\n",
      "the United States Supreme Court noted, “.\n",
      "(('UNCAT', 1.0), ('AXIOM', 0.0))\n",
      "\n",
      "searches conducted outside the judicial process, without prior approval by judge or magistrate, are per se unreasonable under the Fourth Amendment subject only to few specifically established and well-delineated exceptions We are of the opinion that the seizure of these letters and photographs which were not listed on the face of the warrant and therefore seized without prior judicial approval, was proper as coming within just such well-delineated exception; that of “plain view The “plain view” exception was discussed by the United States Supreme Court in Coolidge New Hampshire, Ed.\n",
      "(('UNCAT', 0.748), ('CONCLUSION', 0.212))\n",
      "\n",
      "reh.\n",
      "(('UNCAT', 1.0), ('ISSUE', 0.0))\n",
      "\n",
      "den Ed.\n",
      "(('UNCAT', 1.0), ('ISSUE', 0.0))\n",
      "\n",
      "where that court noted:\n",
      "(('UNCAT', 1.0), ('AXIOM', 0.0))\n",
      "\n",
      "In Coolidge the United States Supreme Court also defined the circumstances which must be present for an object discovered by officers without warrant to be admissible under the “plain view” *exception.\n",
      "(('UNCAT', 0.965), ('ISSUE', 0.022))\n",
      "\n",
      "First, the officers must have prior justification for the intrusion onto the premises being searched Secondly, the incriminating evidence must be inadvertently discovered by the officers while on the premises.\n",
      "(('UNCAT', 0.989), ('LEGAL_TEST', 0.005))\n",
      "\n",
      "Id. at Ed.\n",
      "(('UNCAT', 1.0), ('ISSUE', 0.0))\n",
      "\n",
      "at Ct.\n",
      "(('UNCAT', 1.0), ('LEGAL_TEST', 0.0))\n",
      "\n",
      "at Accord State Richards, E. State Riddick, E. In the case suh judice the officers were justifiably on the premises by virtue of the search warrant issued by disinterested judicial authority, authorizing them to search the mobile home for heroin.\n",
      "(('UNCAT', 0.904), ('CONCLUSION', 0.07))\n",
      "\n",
      "While searching for heroin in the dresser located in the master bedroom, Deputy Parvin saw the pictures and letters which the defendants seek to exclude from evidence.\n",
      "(('UNCAT', 1.0), ('CONCLUSION', 0.0))\n",
      "\n",
      "However, at the time of discovery, Deputy Parvin did not seize the letters.\n",
      "(('UNCAT', 1.0), ('ISSUE', 0.0))\n",
      "\n",
      "Only after Deputy Henderson discovered the heroin in the attached bathroom, and suggested to Deputy Parvin that they needed evidence of the trailer’s ownership, did Deputy Parvin go back to the dresser and confiscate the letters and photographs.\n",
      "(('UNCAT', 0.988), ('CONCLUSION', 0.008))\n",
      "\n",
      "Since Deputy Parvin had inadvertently seen the letters and photographs earlier while conducting an authorized and reasonable search for heroin, subsequent warrantless seizure of these letters and photographs is permissible coming within the “plain view” exception.\n",
      "(('UNCAT', 0.999), ('CONCLUSION', 0.001))\n",
      "\n",
      "Having seen the letters and photographs in place where he was clearly authorized to search for heroin, Deputy Parvin was not required thereafter to forget or ignore the fact that he had seen them.\n",
      "(('UNCAT', 0.996), ('CONCLUSION', 0.003))\n",
      "\n",
      "The items were certainly subject to removal or destruction by defendants if not immediately seized by the officer.\n",
      "(('UNCAT', 0.999), ('ISSUE', 0.0))\n",
      "\n",
      "We are not here concerned with situation where, after discovery of the heroin, the officers commenced an additional search for items of identification.\n",
      "(('UNCAT', 0.999), ('CONCLUSION', 0.001))\n",
      "\n",
      "We also note that pursuant to S. A- the photographs and letters are admissible into evidence.\n",
      "(('UNCAT', 1.0), ('CONCLUSION', 0.0))\n",
      "\n",
      "S. A- in defining what items not named in search warrant may be seized, provides as follows:\n",
      "(('UNCAT', 0.981), ('CONCLUSION', 0.008))\n",
      "\n",
      "“. [ijf in the course of the search the officer inadvertently discovers items not specified in the warrant which are subject to seizure under S. A-, he may also take possession of the items so discovered S. A- provides that an item is subject to seizure if it “[constitutes evidence of the identity of person participating in an offense After *the officers discovered the heroin, the letters and photographs inadvertently seen by Deputy Parvin prior to the heroin’s discovery, are clearly subject to seizure pursuant to S. A- as providing evidence of these defendants’ identities.\n",
      "(('UNCAT', 0.912), ('CONCLUSION', 0.052))\n",
      "\n",
      "We therefore conclude that Judge Stevens properly allowed the letters and photographs into evidence.\n",
      "(('UNCAT', 0.993), ('CONCLUSION', 0.007))\n",
      "\n",
      "The opinion of the Court of Appeals is reversed, the judgment of the trial court is affirmed, and this cause is remanded to the Court of Appeals for further remand to the Superior Court, Onslow County, for issuance of commitments to place the prison sentences into effect.\n",
      "(('UNCAT', 0.947), ('CONCLUSION', 0.035))\n",
      "\n",
      "Reversed and remanded.\n",
      "(('UNCAT', 1.0), ('ISSUE', 0.0))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in cats.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V) Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ROUGE Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall-Oriented Understudy for Gisting Evaluation or <a href=https://en.wikipedia.org/wiki/ROUGE_(metric)>ROUGE</a> is a set of metrics used to score the accuracy of text summarization through machine learning by comparing them with human generated references or \"Gold-Standard\" summaries. ROUGE consists of the following 3 scoring methodologies:\n",
    "\n",
    "1. ROUGE-1: Unigram overlap between the baseline and ML generated summaries\n",
    "2. ROUGE-2: Bigram overlap between the baseline and ML generated summaries\n",
    "3. ROUGE-L: Longest occurring sequence in both baseline and ML generated summaries.\n",
    "\n",
    "ROUGE does not quantify the fluency of the summary but rather assesses the adequacy through counting the number of n-grams in the ML generated summary that are also present in the original source document. Hence it largely depends on the human-inscribed baseline or gold standard summary text.\n",
    "\n",
    "The case document headnotes provided for this project lacks the brevity and simplicity required for a gold standard baseline. Therefore, to test the summaries empirically, the project team penned down 5 case summaries as human-generated gold standard summaries.\n",
    "\n",
    "In addition to standard F1 scores (or 'f'), ROUGE also calculates the following two metrics:\n",
    "\n",
    "p - Precision: measures ML generated summary relevancy. \n",
    "\n",
    "r - Recall: measures the extent of baseline or gold standard summary is being captured by ML generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldnames = ['gold_{}.txt'.format(i) for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_path = 'gold_summaries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gold standard summaries for the first 5 cases are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold-Standard Summary for Case # 1\n",
      "The opinion of the Court of Appeals is reversed, the judgment of the trial court is affirmed, and this cause is remanded to the Court of Appeals.\n",
      "\n",
      "Gold-Standard Summary for Case # 2\n",
      "Defendant plea to declare a miss-trial due to trial court Judge miss-conduct and violation of defendant’s constitutional rights was over-ruled after Court’s consideration of the entire record in which no error were found warranting new trial.\n",
      "\n",
      "Gold-Standard Summary for Case # 3\n",
      "Court of Appeals affirmed the decision of the Commission to allow roll in of the past under collection during the prior year to offset an over collection during the following annual period.\n",
      "\n",
      "Gold-Standard Summary for Case # 4\n",
      "For the reasons stated the decision of the Court of Appeals as it relates to Southern Bell is reversed.\n",
      "\n",
      "Gold-Standard Summary for Case # 5\n",
      "Defendant plea that due process of law was denied was rejected by the Court, as it observed that defendant sentencing hearing was properly conducted and had a fair trial free from prejudicial error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gold-standard summaries\n",
    "gold_summaries = []\n",
    "for i, file in enumerate(goldnames):\n",
    "    with open(os.path.join(gold_path, file)) as f:\n",
    "        gold_summaries.append(f.read())\n",
    "    print('Gold-Standard Summary for Case # {}'.format(i+1))\n",
    "    print(gold_summaries[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average ROUGE-1, ROUGE-2 and ROUGE-l scores were calculated for the Gensim, BertSum and GPT-2 summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "scores_gen = rouge.get_scores(gensim_summaries, gold_summaries, avg=True)\n",
    "scores_bert = rouge.get_scores(BertSum_summaries, gold_summaries, avg=True)\n",
    "scores_gpt2 = rouge.get_scores(gpt2_summaries, gold_summaries, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE 1 (Unigram): GPT-2 has the highest score as compared to the other two models, though recall seems to be higher in Gensim as compared to GPT2. This could be due to the extractive nature of Gensim summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 Scores for Each Model (Over 5 Cases):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gensim</th>\n",
       "      <th>BertSum</th>\n",
       "      <th>GPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>f</td>\n",
       "      <td>0.206432</td>\n",
       "      <td>0.210068</td>\n",
       "      <td>0.272178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>p</td>\n",
       "      <td>0.168618</td>\n",
       "      <td>0.176026</td>\n",
       "      <td>0.336241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>r</td>\n",
       "      <td>0.308444</td>\n",
       "      <td>0.268419</td>\n",
       "      <td>0.253603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gensim   BertSum      GPT2\n",
       "f  0.206432  0.210068  0.272178\n",
       "p  0.168618  0.176026  0.336241\n",
       "r  0.308444  0.268419  0.253603"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average ROUGE-1 Scores for Each Model (Over 5 Cases):\")\n",
    "\n",
    "pd.DataFrame({\"Gensim\": scores_gen['rouge-1'], \"BertSum\": scores_bert['rouge-1'], \"GPT2\": scores_gpt2['rouge-1']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE 2 (Bigram): Gensim is ranked higher than the other two models, possibly due to the extractive summarization causing a higher overlap of bigrams with baseline summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-2 Scores for Each Model (Over 5 Cases):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gensim</th>\n",
       "      <th>BertSum</th>\n",
       "      <th>GPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>f</td>\n",
       "      <td>0.047046</td>\n",
       "      <td>0.017203</td>\n",
       "      <td>0.046218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>p</td>\n",
       "      <td>0.041223</td>\n",
       "      <td>0.014201</td>\n",
       "      <td>0.042883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>r</td>\n",
       "      <td>0.063662</td>\n",
       "      <td>0.021836</td>\n",
       "      <td>0.050124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gensim   BertSum      GPT2\n",
       "f  0.047046  0.017203  0.046218\n",
       "p  0.041223  0.014201  0.042883\n",
       "r  0.063662  0.021836  0.050124"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average ROUGE-2 Scores for Each Model (Over 5 Cases):\")\n",
    "\n",
    "pd.DataFrame({\"Gensim\": scores_gen['rouge-2'], \"BertSum\": scores_bert['rouge-2'], \"GPT2\": scores_gpt2['rouge-2']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE l (Longest Sequence): GPT2 scored higher than the other two models, though Gensim again scored highest in the recall metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-l Scores for Each Model (Over 5 Cases):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gensim</th>\n",
       "      <th>BertSum</th>\n",
       "      <th>GPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>f</td>\n",
       "      <td>0.137041</td>\n",
       "      <td>0.133738</td>\n",
       "      <td>0.181614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>p</td>\n",
       "      <td>0.117773</td>\n",
       "      <td>0.112548</td>\n",
       "      <td>0.220833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>r</td>\n",
       "      <td>0.196311</td>\n",
       "      <td>0.173070</td>\n",
       "      <td>0.177042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gensim   BertSum      GPT2\n",
       "f  0.137041  0.133738  0.181614\n",
       "p  0.117773  0.112548  0.220833\n",
       "r  0.196311  0.173070  0.177042"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average ROUGE-l Scores for Each Model (Over 5 Cases):\")\n",
    "\n",
    "pd.DataFrame({\"Gensim\": scores_gen['rouge-l'], \"BertSum\": scores_bert['rouge-l'], \"GPT2\": scores_gpt2['rouge-l']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Intuitive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though ROUGE scores are extremely useful in many contexts, they rely on strict adherence to sentence length and reproducability of ngrams, and cannot necessarily judge how well a summary captures the essence of a text. For the 5 cases evaluated with ROUGE, an intuitive look at the various summaries shows how summaries with higher ROUGE scores are not neccesarily \"better\" summaries.  \n",
    "\n",
    "For each of the 5 cases, the Gold-Standard (human) summary is printed out, followed by the Gensim, BertSum and GPT-2 summaries. Observations are then made comparing the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold-Standard Summary for Case # 1\n",
      "The opinion of the Court of Appeals is reversed, the judgment of the trial court is affirmed, and this cause is remanded to the Court of Appeals.\n",
      "\n",
      "Gensim Extractive Summary for Case # 1\n",
      "searches conducted outside the judicial process, without prior approval by judge or magistrate, are per se unreasonable under the Fourth Amendment subject only to few specifically established and well-delineated exceptions We are of the opinion that the seizure of these letters and photographs which were not listed on the face of the warrant and therefore seized without prior judicial approval, was proper as coming within just such well-delineated exception; that of “plain view The “plain view” exception was discussed by the United States Supreme Court in Coolidge New Hampshire, Ed. Ct.\n",
      "\n",
      "BertSum Summary for Case # 1\n",
      "danny cevallos : the only question for review is the admissibility of letters and photographs seized by deputies of the onslow county sheriff 's department during search of the mobile home occupied by the defendants<q>he says the seizure of these letters and\n",
      "\n",
      "\n",
      "GPT2 Summary for Case # 1\n",
      " The Court of Appeals for the Ninth Circuit, in its opinion, held that the search of the mobile home of the defendants was not per se unreasonable under the Fourth Amendment\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print('Gold-Standard Summary for Case # {}'.format(i+1))\n",
    "print(gold_summaries[i])\n",
    "print()\n",
    "print('Gensim Extractive Summary for Case # {}'.format(i+1))\n",
    "print(gensim_summaries[i])\n",
    "print()\n",
    "print('BertSum Summary for Case # {}'.format(i+1))\n",
    "print(BertSum_summaries[i])\n",
    "print()\n",
    "print('GPT2 Summary for Case # {}'.format(i+1))\n",
    "print(gpt2_summaries[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1 Observation:** Gensim provides a very good summary, despite its low ROUGE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold-Standard Summary for Case # 2\n",
      "Defendant plea to declare a miss-trial due to trial court Judge miss-conduct and violation of defendant’s constitutional rights was over-ruled after Court’s consideration of the entire record in which no error were found warranting new trial.\n",
      "\n",
      "Gensim Extractive Summary for Case # 2\n",
      "In that case, the Court held that the trial judge did not commit reversible error by permitting further examination and challenge of juror by the State after the jury was impaneled, when it was discovered that the juror worked with the wife of one of the defendants.\n",
      "\n",
      "BertSum Summary for Case # 2\n",
      "the trial judge did not commit reversible error by permitting further examination and challenge of juror by the state after the jury was impaneled<q>mrs trogdon testified on voir dire that after she had seen defendant at the restaurant in september\n",
      "\n",
      "\n",
      "GPT2 Summary for Case # 2\n",
      " The State was not required to prove the charge of rape in order to convict defendant of burglary\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "print('Gold-Standard Summary for Case # {}'.format(i+1))\n",
    "print(gold_summaries[i])\n",
    "print()\n",
    "print('Gensim Extractive Summary for Case # {}'.format(i+1))\n",
    "print(gensim_summaries[i])\n",
    "print()\n",
    "print('BertSum Summary for Case # {}'.format(i+1))\n",
    "print(BertSum_summaries[i])\n",
    "print()\n",
    "print('GPT2 Summary for Case # {}'.format(i+1))\n",
    "print(gpt2_summaries[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2 Observation:** Though GPT-2 has the highest ROUGE scores, its summary draws from some intermediate context of the text, and not from its conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold-Standard Summary for Case # 3\n",
      "Court of Appeals affirmed the decision of the Commission to allow roll in of the past under collection during the prior year to offset an over collection during the following annual period.\n",
      "\n",
      "Gensim Extractive Summary for Case # 3\n",
      "We have concluded, however, that rate adjustment pursuant to an annual CTR “true up” is not change in fixed general rate, and thus the rate adjustment in this case which allowed NCNG to offset its overcollection by its previous undercollection does not *constitute retroactive rate making prohibited by Utilities Commission Edmisten, E.\n",
      "\n",
      "BertSum Summary for Case # 3\n",
      "the court of appeals upheld the use of formula known as the volume variation adjustment factor which provided for the yearly \" true up \" of natural gas in manner identical to that of the ctr<q>in the case , ncng can not know the actual rate\n",
      "\n",
      "\n",
      "GPT2 Summary for Case # 3\n",
      "\n",
      "The Court of Appeals held that the Commission's decision to reduce the amount of the “true up” of NCNG's past undercollection by NCNG was not change in fixed general rate, and therefore the Commission's decision to reduce the\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "print('Gold-Standard Summary for Case # {}'.format(i+1))\n",
    "print(gold_summaries[i])\n",
    "print()\n",
    "print('Gensim Extractive Summary for Case # {}'.format(i+1))\n",
    "print(gensim_summaries[i])\n",
    "print()\n",
    "print('BertSum Summary for Case # {}'.format(i+1))\n",
    "print(BertSum_summaries[i])\n",
    "print()\n",
    "print('GPT2 Summary for Case # {}'.format(i+1))\n",
    "print(gpt2_summaries[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 3 Observation:** Gensim and BertSum provide fair summaries, while GPT-2 does not capture the essence of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold-Standard Summary for Case # 4\n",
      "For the reasons stated the decision of the Court of Appeals as it relates to Southern Bell is reversed.\n",
      "\n",
      "Gensim Extractive Summary for Case # 4\n",
      "“Authority to do an act on the principal’s behalf does not ordinarily carry with it an implied authority to talk about it afterwards Stansbury, supra, at [] Application of the above principles to the facts in this case leads us to conclude that the statements allegedly made to plaintiff by defendant’s agent, Rochelle, were erroneously admitted into evidence.\n",
      "\n",
      "BertSum Summary for Case # 4\n",
      "the dispositive question on this appeal is whether statements allegedly made to plaintiff by defendant 's agent rochelle were properly admissible into evidence as the admissions of southern bell<q>if there is no competent evidence that an agent has authority to speak for his\n",
      "\n",
      "\n",
      "GPT2 Summary for Case # 4\n",
      " The majority's opinion is correct\n"
     ]
    }
   ],
   "source": [
    "i=3\n",
    "print('Gold-Standard Summary for Case # {}'.format(i+1))\n",
    "print(gold_summaries[i])\n",
    "print()\n",
    "print('Gensim Extractive Summary for Case # {}'.format(i+1))\n",
    "print(gensim_summaries[i])\n",
    "print()\n",
    "print('BertSum Summary for Case # {}'.format(i+1))\n",
    "print(BertSum_summaries[i])\n",
    "print()\n",
    "print('GPT2 Summary for Case # {}'.format(i+1))\n",
    "print(gpt2_summaries[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 4 Observation:** Gensim generated a great summary. BertSum simply printed out the first two sentences of the text. The GPT-2 summary is too general and uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold-Standard Summary for Case # 5\n",
      "Defendant plea that due process of law was denied was rejected by the Court, as it observed that defendant sentencing hearing was properly conducted and had a fair trial free from prejudicial error.\n",
      "\n",
      "Gensim Extractive Summary for Case # 5\n",
      "[] Defendant’s first argument is that he was denied due process of law when the trial judge refused to grant him an evidentiary *hearing upon his motion to dismiss all of the charges.\n",
      "\n",
      "BertSum Summary for Case # 5\n",
      "defense counsel produced statistics tending to show that other defendants had charges dismissed when the prosecuting witness so desired<q>if these statistics alone were enough to establish denial of equal protection , mandatory rule would be created requiring the district attorney to dismiss charges in all cases where\n",
      "\n",
      "\n",
      "GPT2 Summary for Case # 5\n",
      " The trial judge stated that the defendant was not entitled to counsel during the sentencing phase\n"
     ]
    }
   ],
   "source": [
    "i=4\n",
    "print('Gold-Standard Summary for Case # {}'.format(i+1))\n",
    "print(gold_summaries[i])\n",
    "print()\n",
    "print('Gensim Extractive Summary for Case # {}'.format(i+1))\n",
    "print(gensim_summaries[i])\n",
    "print()\n",
    "print('BertSum Summary for Case # {}'.format(i+1))\n",
    "print(BertSum_summaries[i])\n",
    "print()\n",
    "print('GPT2 Summary for Case # {}'.format(i+1))\n",
    "print(gpt2_summaries[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 5 Observation:** Gensim generated an argument from the middle of the case document as the summary rather than the conclusion.  BertSum and GPT-2 both generated information that cannot be considered case summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) Conclusion and Further Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Successes and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*i. Seq2Seq*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq model was the least successful of the attempted methods. Due to limitations in time and resources, adequate training time could not be allocated to produce a model that generates coherent predicitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ii. Gensim Extractive Summarization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, while many of the Gensim extractive summaries successfully identified the most important sentences, they did present some limitations, particularly those mentioned at the beginning of section III. In particular, some Gensim summaries were observed to have included some irrelevant parts of text while omitting other important parts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*iii. BertSum*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertSum succeeded, for the most part, in generating coherent sentences. However, it was extremely inconsistent in terms of capturing the essence of the text vs. summarizing less important (or even conflicting) areas of the text. This is likely due to the fact that the model is trained exclusively on news text from CNN and Daily Mail, as opposed to texts more similar to North Carolina case law.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*iv. GPT-2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 was also extremely hit-or-miss. While its more successful summaries almost perfectly captured the essence of the text succinctly and comprehensively, some summaries were either far too general or missed the most important aspects of the text. This is likely because of how general the training data is for GPT-2, combined with its configuration limitations (including max word count and temperature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations in Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most difficult parts of this project was determining a viable (let alone scalable) evaluation method for summaries. Though ROUGE scores are useful in some contexts, they failed to adequately measure how well the summaries captured the essence of the cases. This was compounded by the fact that ROUGE scores could only be calculated for 5 cases, since only 5 Gold-Standard summaries could be compiled. Ideally, Gold-Standard summaries would be created by humans for ALL available cases. These humans should have some baseline of legal expertise in order to confidently capture the essence of each legal text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the modelling approaches used, extractive summarization with Gensim provided the best combination of accuracy, coverage and consistency. It could prove useful for searching through or filtering cases, or getting a general idea of a case's contents. However, the model is not specifically trained for legal text. Furthermore, the nature of extractive summarization means that the summaries are simply pieces of the text that are deemed \"most\" important, but there could be other parts of the text that are not included that are important as well. It is therefore recommended that these summaries be relied upon only as a helpful tool, and not for drawing any conclusions about the context or scope of the case itself. It is very likely that a fully-trained GPT-2 model on the legal corpus could produce more reliable and comprehensive results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the case law summarization to an adequate level the following steps needs to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. *Gold Standard Summaries:*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models may need to be trained on case law documents which have succinct and accurate summaries transcribed by humans to be used as the gold standard. This will help the models to learn the specific semantics and contextual and hierarchical features of the legal documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. *Extractive versus Abstractive approach:*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since extractive summarization models are relatively simplistic, it may be prudent to look into enhancing an extractive pre-trained model to generate case law summaries in first phase. BertSum may be fine-tuned to generate extractive summaries. \n",
    "  \n",
    "For abstractive summarization there may be a need to investigate Google’s T5 which is one of the most advanced NLP pre-trained models. One key feature contrast with BERT, an encoder blocks dominated model, and GPT-2, which primarily consists of decoder blocks, is that T5 uses both. T5 is trained on a 7TB dataset and supposedly can generate multiple versions of text summarization from the same source text with varying vocabulary and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. *Seq2Seq Model*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given more time and computational resources, a Seq2Seq model could be trained using more data, more parameters, a more complex network and could train for a longer period of time. This could potentially yield usable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII) References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case Law Dataset from Caselaw Access Project**  \n",
    "https://case.law/\n",
    "\n",
    "**Gensim Extractive Summarizer**  \n",
    "https://radimrehurek.com/gensim/summarization/summariser.html\n",
    "\n",
    "**TextRank Algorithm**  \n",
    "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "\n",
    "**GloVe Wikipedia 2014 + Gigaworld 5 Embeddings**  \n",
    "https://github.com/stanfordnlp/GloVe\n",
    "\n",
    "**Seq2Seq Architectures**  \n",
    "https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/\n",
    "\n",
    "**BertSum**  \n",
    "https://arxiv.org/abs/1903.10318\n",
    "\n",
    "**CNN/Daily Mail Dataset**  \n",
    "https://cs.nyu.edu/~kcho/DMQA/\n",
    "\n",
    "**PreSumm Implementation of BertSum**  \n",
    "https://github.com/nlpyang/PreSumm\n",
    "\n",
    "**Blackstone Legal Text Categorization**  \n",
    "https://github.com/ICLRandD/Blackstone\n",
    "\n",
    "**GPT-2**  \n",
    "https://openai.com/blog/better-language-models/\n",
    "\n",
    "**HuggingFace GPT-2 Implementation**  \n",
    "https://huggingface.co/transformers/model_doc/gpt2.html\n",
    "\n",
    "**ROUGE Score Summary Evaluation**  \n",
    "https://www.aclweb.org/anthology/W04-1013/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
